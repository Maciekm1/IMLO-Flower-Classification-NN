{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "EPOCH_PATIENCE = 11\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_CLASSES = 102\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.00001\n",
    "\n",
    "L1_NORM = 0.0001\n",
    "ROOT = \"/Users/maciek/cnn_data\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FavConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FavConvolutionalNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=256, kernel_size=3)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=512, out_channels=2048, kernel_size=3)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(2048)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048 * 5 * 5, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.drop = nn.Dropout(p=0.7)\n",
    "        \n",
    "        self.out = nn.Linear(1024, NUM_CLASSES)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = x.view(-1, 2048 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.drop(x)\n",
    "        \n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.4330, 0.3819, 0.2964]\n",
    "std = [0.2545, 0.2044, 0.2163]\n",
    "\n",
    "train_transform_1 = Compose([\n",
    "    Resize((112,112)),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomVerticalFlip(),\n",
    "    RandomRotation(30),\n",
    "    ToTensor(),\n",
    "    Normalize(mean, std)\n",
    "])\n",
    "train_transform_2 = Compose([\n",
    "    Resize((112, 112)),\n",
    "    ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2),\n",
    "    RandomPerspective(distortion_scale=0.4, p=0.6),\n",
    "    ToTensor(),\n",
    "    Normalize(mean, std)\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((112, 112)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = datasets.Flowers102(root=ROOT, split='train', download=True, transform=test_transform)\n",
    "train_data_2 = datasets.Flowers102(root=ROOT, split='train', download=True, transform=train_transform_1)\n",
    "train_data_3 = datasets.Flowers102(root=ROOT, split='train', download=True, transform=train_transform_2)\n",
    "train_data = train_data_1 + train_data_2 + train_data_3\n",
    "\n",
    "val_data = datasets.Flowers102(root=ROOT, split='val', download=True, transform=test_transform)\n",
    "test_data = datasets.Flowers102(root=ROOT, split='test', download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FavConvolutionalNetwork().to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.2, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm(model):\n",
    "    l1_regularization = torch.tensor(0.).to(DEVICE)\n",
    "    for param in model.parameters():\n",
    "        l1_regularization += torch.norm(param, 1)\n",
    "    return l1_regularization    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/150] train_loss: 1660.575 val_loss: 4.524 val_acc: 4.7%\n",
      "[1/150] train_loss: 1455.121 val_loss: 4.472 val_acc: 6.7%\n",
      "[2/150] train_loss: 1280.354 val_loss: 4.367 val_acc: 6.7%\n",
      "[3/150] train_loss: 1144.678 val_loss: 4.358 val_acc: 10.3%\n",
      "[4/150] train_loss: 1033.490 val_loss: 4.213 val_acc: 9.5%\n",
      "[5/150] train_loss: 954.427 val_loss: 4.050 val_acc: 14.1%\n",
      "[6/150] train_loss: 882.001 val_loss: 3.968 val_acc: 13.9%\n",
      "[7/150] train_loss: 827.134 val_loss: 3.759 val_acc: 17.3%\n",
      "[8/150] train_loss: 787.524 val_loss: 3.690 val_acc: 16.8%\n",
      "[9/150] train_loss: 744.214 val_loss: 3.514 val_acc: 21.7%\n",
      "[10/150] train_loss: 716.372 val_loss: 3.529 val_acc: 21.0%\n",
      "[11/150] train_loss: 695.898 val_loss: 3.353 val_acc: 20.9%\n",
      "[12/150] train_loss: 675.973 val_loss: 3.301 val_acc: 25.2%\n",
      "[13/150] train_loss: 658.441 val_loss: 3.117 val_acc: 26.4%\n",
      "[14/150] train_loss: 647.280 val_loss: 3.075 val_acc: 28.4%\n",
      "[15/150] train_loss: 631.418 val_loss: 2.945 val_acc: 31.6%\n",
      "[16/150] train_loss: 631.107 val_loss: 2.896 val_acc: 31.8%\n",
      "[17/150] train_loss: 621.447 val_loss: 2.804 val_acc: 31.2%\n",
      "[18/150] train_loss: 620.575 val_loss: 2.661 val_acc: 36.3%\n",
      "[19/150] train_loss: 621.520 val_loss: 2.587 val_acc: 40.4%\n",
      "[20/150] train_loss: 613.883 val_loss: 2.484 val_acc: 38.9%\n",
      "[21/150] train_loss: 617.007 val_loss: 2.452 val_acc: 40.6%\n",
      "[22/150] train_loss: 611.144 val_loss: 2.268 val_acc: 45.1%\n",
      "[23/150] train_loss: 609.766 val_loss: 2.223 val_acc: 44.8%\n",
      "[24/150] train_loss: 606.779 val_loss: 2.218 val_acc: 43.3%\n",
      "[25/150] train_loss: 600.092 val_loss: 2.116 val_acc: 48.6%\n",
      "[26/150] train_loss: 600.112 val_loss: 1.960 val_acc: 52.5%\n",
      "[27/150] train_loss: 596.932 val_loss: 1.985 val_acc: 51.5%\n",
      "[28/150] train_loss: 592.501 val_loss: 1.972 val_acc: 52.2%\n",
      "[29/150] train_loss: 583.696 val_loss: 1.858 val_acc: 54.5%\n",
      "[30/150] train_loss: 578.782 val_loss: 1.821 val_acc: 54.0%\n",
      "[31/150] train_loss: 564.936 val_loss: 1.844 val_acc: 53.3%\n",
      "[32/150] train_loss: 557.158 val_loss: 1.741 val_acc: 56.2%\n",
      "[33/150] train_loss: 556.536 val_loss: 1.720 val_acc: 57.1%\n",
      "[34/150] train_loss: 549.069 val_loss: 1.765 val_acc: 57.3%\n",
      "[35/150] train_loss: 538.372 val_loss: 1.713 val_acc: 57.7%\n",
      "[36/150] train_loss: 530.518 val_loss: 1.665 val_acc: 57.8%\n",
      "[37/150] train_loss: 521.310 val_loss: 1.637 val_acc: 58.3%\n",
      "[38/150] train_loss: 515.771 val_loss: 1.629 val_acc: 58.5%\n",
      "[39/150] train_loss: 515.060 val_loss: 1.614 val_acc: 59.6%\n",
      "[40/150] train_loss: 509.561 val_loss: 1.613 val_acc: 61.1%\n",
      "[41/150] train_loss: 499.523 val_loss: 1.575 val_acc: 61.3%\n",
      "[42/150] train_loss: 491.454 val_loss: 1.724 val_acc: 58.3%\n",
      "[43/150] train_loss: 492.875 val_loss: 1.578 val_acc: 60.3%\n",
      "[44/150] train_loss: 490.873 val_loss: 1.600 val_acc: 61.2%\n",
      "[45/150] train_loss: 486.549 val_loss: 1.614 val_acc: 60.1%\n",
      "[46/150] train_loss: 474.514 val_loss: 1.575 val_acc: 61.1%\n",
      "[47/150] train_loss: 469.247 val_loss: 1.608 val_acc: 61.6%\n",
      "[48/150] train_loss: 457.379 val_loss: 1.428 val_acc: 66.4%\n",
      "[49/150] train_loss: 436.144 val_loss: 1.391 val_acc: 66.8%\n",
      "[50/150] train_loss: 418.399 val_loss: 1.379 val_acc: 66.7%\n",
      "[51/150] train_loss: 403.280 val_loss: 1.379 val_acc: 66.6%\n",
      "[52/150] train_loss: 388.845 val_loss: 1.393 val_acc: 66.4%\n",
      "[53/150] train_loss: 376.915 val_loss: 1.366 val_acc: 67.6%\n",
      "[54/150] train_loss: 367.691 val_loss: 1.356 val_acc: 67.5%\n",
      "[55/150] train_loss: 358.331 val_loss: 1.349 val_acc: 67.8%\n",
      "[56/150] train_loss: 349.513 val_loss: 1.336 val_acc: 68.3%\n",
      "[57/150] train_loss: 339.682 val_loss: 1.342 val_acc: 68.2%\n",
      "[58/150] train_loss: 331.970 val_loss: 1.353 val_acc: 66.9%\n",
      "[59/150] train_loss: 325.628 val_loss: 1.343 val_acc: 67.7%\n",
      "[60/150] train_loss: 320.118 val_loss: 1.355 val_acc: 67.9%\n",
      "[61/150] train_loss: 312.681 val_loss: 1.333 val_acc: 68.1%\n",
      "[62/150] train_loss: 307.346 val_loss: 1.342 val_acc: 67.5%\n",
      "[63/150] train_loss: 301.756 val_loss: 1.320 val_acc: 67.5%\n",
      "[64/150] train_loss: 296.290 val_loss: 1.336 val_acc: 67.3%\n",
      "[65/150] train_loss: 292.585 val_loss: 1.327 val_acc: 68.0%\n",
      "[66/150] train_loss: 288.840 val_loss: 1.327 val_acc: 68.1%\n",
      "[67/150] train_loss: 284.378 val_loss: 1.332 val_acc: 67.7%\n",
      "[68/150] train_loss: 280.967 val_loss: 1.323 val_acc: 66.9%\n",
      "[69/150] train_loss: 278.409 val_loss: 1.324 val_acc: 67.5%\n",
      "[70/150] train_loss: 275.055 val_loss: 1.321 val_acc: 67.3%\n",
      "[71/150] train_loss: 271.136 val_loss: 1.310 val_acc: 69.1%\n",
      "[72/150] train_loss: 267.571 val_loss: 1.303 val_acc: 68.9%\n",
      "[73/150] train_loss: 264.655 val_loss: 1.305 val_acc: 68.6%\n",
      "[74/150] train_loss: 262.267 val_loss: 1.302 val_acc: 68.9%\n",
      "[75/150] train_loss: 261.250 val_loss: 1.314 val_acc: 68.1%\n",
      "[76/150] train_loss: 259.010 val_loss: 1.310 val_acc: 67.9%\n",
      "[77/150] train_loss: 257.312 val_loss: 1.307 val_acc: 68.2%\n",
      "[78/150] train_loss: 256.062 val_loss: 1.305 val_acc: 68.1%\n",
      "[79/150] train_loss: 253.355 val_loss: 1.299 val_acc: 67.7%\n",
      "[80/150] train_loss: 252.403 val_loss: 1.295 val_acc: 68.3%\n",
      "[81/150] train_loss: 251.309 val_loss: 1.299 val_acc: 68.3%\n",
      "[82/150] train_loss: 250.519 val_loss: 1.304 val_acc: 67.9%\n",
      "[83/150] train_loss: 248.935 val_loss: 1.300 val_acc: 68.0%\n",
      "[84/150] train_loss: 248.215 val_loss: 1.304 val_acc: 68.1%\n",
      "[85/150] train_loss: 247.563 val_loss: 1.306 val_acc: 67.8%\n",
      "[86/150] train_loss: 246.046 val_loss: 1.300 val_acc: 67.6%\n",
      "[87/150] train_loss: 245.706 val_loss: 1.295 val_acc: 67.5%\n",
      "[88/150] train_loss: 243.888 val_loss: 1.294 val_acc: 68.4%\n",
      "[89/150] train_loss: 244.112 val_loss: 1.298 val_acc: 67.5%\n",
      "[90/150] train_loss: 244.148 val_loss: 1.292 val_acc: 67.8%\n",
      "[91/150] train_loss: 242.617 val_loss: 1.295 val_acc: 67.5%\n",
      "[92/150] train_loss: 242.209 val_loss: 1.295 val_acc: 68.0%\n",
      "[93/150] train_loss: 242.950 val_loss: 1.296 val_acc: 68.2%\n",
      "[94/150] train_loss: 241.478 val_loss: 1.303 val_acc: 67.9%\n",
      "[95/150] train_loss: 241.309 val_loss: 1.304 val_acc: 68.1%\n",
      "[96/150] train_loss: 241.644 val_loss: 1.295 val_acc: 68.4%\n",
      "[97/150] train_loss: 240.880 val_loss: 1.297 val_acc: 68.9%\n",
      "[98/150] train_loss: 241.124 val_loss: 1.305 val_acc: 68.0%\n",
      "[99/150] train_loss: 240.814 val_loss: 1.293 val_acc: 68.2%\n",
      "[100/150] train_loss: 240.350 val_loss: 1.300 val_acc: 68.0%\n",
      "Stopping early at epoch 101 due to overfitting.\n",
      "Training Took: 50.63213610251744 minutes!\n"
     ]
    }
   ],
   "source": [
    "# TRAINING AND VALIDATION LOOP\n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Variables To Tracks Things\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = EPOCH_PATIENCE  # How many epochs to wait after val loss has stopped improving\n",
    "min_val_loss = float('inf')\n",
    "stale_epochs = 0  # Counter for epochs without improvement\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "\n",
    "    # Train\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for b, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "        X_train, y_train = X_train.to(DEVICE), y_train.to(DEVICE)\n",
    "        y_pred = model(X_train)  # get predicted values from the training set. Not flattened 2D\n",
    "        loss = criterion(y_pred, y_train)  # Compare the predictions to correct answers in y_train\n",
    "\n",
    "        predicted = torch.max(y_pred.data, 1)[\n",
    "            1]  # add up the number of correct predictions. Indexed off the first point\n",
    "        batch_corr = (predicted == y_train).sum()  # how many we got correct from this batch. True = 1, False=0, sum those up\n",
    "        trn_corr += batch_corr  # keep track as we go along in training.\n",
    "\n",
    "        l1_loss = L1_NORM * l1_norm(model)\n",
    "        total_loss = l1_loss + loss\n",
    "        running_loss += total_loss.item() * X_train.size(0)\n",
    "        # Update our parameters\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    # Test\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            X_test, y_test = X_test.to(DEVICE), y_test.to(DEVICE)\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            total += y_test.size(0)\n",
    "            val_loss += criterion(y_val, y_test).item()  # Sum up the loss from each batch\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Calculate the average loss\n",
    "\n",
    "    test_losses.append(avg_val_loss)\n",
    "    test_correct.append(tst_corr)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        stale_epochs = 0  # Reset the stale epochs counter\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        stale_epochs += 1  # Increment the stale epochs counter\n",
    "        if stale_epochs >= patience:\n",
    "            print(f'Stopping early at epoch {i} due to overfitting.')\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break  # Break out of the loop\n",
    "\n",
    "    print(f'[{i}/{EPOCHS}] train_loss: {avg_train_loss:.3f} val_loss: {avg_val_loss:.3f} val_acc: {(tst_corr/total)*100:.1f}%')\n",
    "\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'Training Took: {total / 60} minutes!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FavConvolutionalNetwork:\n\tUnexpected key(s) in state_dict: \"conv5.weight\", \"conv5.bias\", \"batch_norm5.weight\", \"batch_norm5.bias\", \"batch_norm5.running_mean\", \"batch_norm5.running_var\", \"batch_norm5.num_batches_tracked\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m model_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_weights\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FavConvolutionalNetwork:\n\tUnexpected key(s) in state_dict: \"conv5.weight\", \"conv5.bias\", \"batch_norm5.weight\", \"batch_norm5.bias\", \"batch_norm5.running_mean\", \"batch_norm5.running_var\", \"batch_norm5.num_batches_tracked\". "
     ]
    }
   ],
   "source": [
    "# LOADING THE MODEL\n",
    "\n",
    "model = FavConvolutionalNetwork()\n",
    "model.eval()\n",
    "\n",
    "model_weights = torch.load('model_weights.pth')\n",
    "model.load_state_dict(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert GPU tensors to CPU tensors, detach them from the computation graph, and then to NumPy arrays\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_losses \u001b[38;5;241m=\u001b[39m [tl \u001b[38;5;28;01mfor\u001b[39;00m tl \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_losses\u001b[49m]\n\u001b[1;32m      3\u001b[0m test_losses \u001b[38;5;241m=\u001b[39m [tl \u001b[38;5;28;01mfor\u001b[39;00m tl \u001b[38;5;129;01min\u001b[39;00m test_losses]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Now you can plot using matplotlib\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert GPU tensors to CPU tensors, detach them from the computation graph, and then to NumPy arrays\n",
    "train_losses = [tl for tl in train_losses]\n",
    "test_losses = [tl for tl in test_losses]\n",
    "\n",
    "# Now you can plot using matplotlib\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(test_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss at Epoch\")\n",
    "plt.legend()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([t\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_correct\u001b[49m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([t\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m test_correct], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy at the end of each Epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_correct' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot([t.cpu()/30 for t in train_correct], label=\"Training Accuracy\")\n",
    "plt.plot([t.cpu()/10 for t in test_correct], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy at the end of each Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the best model state\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m test_load_everything \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtest_data\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize lists to store predictions and true labels for evaluation\u001b[39;00m\n\u001b[1;32m      7\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "# Load the best model state\n",
    "test_load_everything = DataLoader(test_data, batch_size=512, shuffle=False)\n",
    "\n",
    "# Initialize lists to store predictions and true labels for evaluation\n",
    "final_predictions = []\n",
    "final_true_labels = []\n",
    "\n",
    "# Evaluation loop\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_load_everything:\n",
    "        X_test, y_test = X_test.to(DEVICE), y_test.to(DEVICE)\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.max(y_val.data, 1)[1]\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        total += y_test.size(0)\n",
    "        \n",
    "        # Store predictions and true labels\n",
    "        final_predictions.extend(predicted.view(-1).cpu().numpy())\n",
    "        final_true_labels.extend(y_test.view(-1).cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "predictions = np.array(final_predictions)\n",
    "true_labels = np.array(final_true_labels)\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "recall = recall_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "f1 = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Accuracy of the model on the test set: {accuracy * 100:.2f}%')\n",
    "print(f\"Final Precision: {precision}\")\n",
    "print(f\"Final Recall: {recall}\")\n",
    "print(f\"Final F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TESTING MODEL\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m----> 5\u001b[0m test_load_everything \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtest_data\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# TESTING MODEL\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_load_everything = DataLoader(test_data, batch_size=512, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_load_everything:\n",
    "        X_test, y_test = X_test.to(DEVICE), y_test.to(DEVICE)\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.max(y_val, 1)[1]\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        total += y_test.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the model on the test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL WEIGHTS\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
