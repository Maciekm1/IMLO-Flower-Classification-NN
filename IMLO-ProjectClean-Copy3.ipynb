{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "EPOCH_PATIENCE = 10\n",
    "BATCH_SIZE = 32\n",
    "LR = 2.2e-2\n",
    "NUM_CLASSES = 102\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 200, kernel_size=(5,5), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(6, 6)),\n",
    "            nn.BatchNorm2d(200),\n",
    "            \n",
    "            nn.Conv2d(200, 400, kernel_size=(5,5), stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(8, 8)),\n",
    "            nn.BatchNorm2d(400),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(6400, 6000),\n",
    "            nn.ReLU(),\n",
    "    \n",
    "            nn.Linear(6000, 102)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transform(epoch):\n",
    "    new_transform = copy.deepcopy(train_transform_1)\n",
    "    \n",
    "    # Basic transformations\n",
    "    if epoch > 10:\n",
    "        new_transform.transforms.insert(0, RandomHorizontalFlip(p=0.5))\n",
    "    if epoch > 15:\n",
    "        new_transform.transforms.insert(0, RandomVerticalFlip(p=0.5))\n",
    "        new_transform.transforms.insert(1, RandomRotation(degrees=15))  # Adjusted rotation\n",
    "    if epoch > 20:\n",
    "        new_transform.transforms[1] = RandomRotation(degrees=45)  # Gradual increase in rotation\n",
    "    # Intermediate transformations\n",
    "    if epoch > 30:\n",
    "        new_transform.transforms.insert(4, RandomAffine(degrees=0, translate=(0.1, 0.1)))  # Gradual affine\n",
    "    if epoch > 40:\n",
    "        new_transform.transforms.insert(5, RandomPerspective(distortion_scale=0.2, p=0.2))  # Gradual perspective\n",
    "    \n",
    "    return new_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_1 = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_2 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "    RandomRotation(degrees=90),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_3 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)),\n",
    "    RandomRotation(degrees=45),\n",
    "    RandomVerticalFlip(p=0.8),\n",
    "    RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_1)\n",
    "train_data_2 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_2)\n",
    "train_data_3 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_3)\n",
    "train_data = train_data_1 + train_data_2 + train_data_2 + train_data_2 + train_data_3 + train_data_3\n",
    "val_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='val', download=True, transform=val_transform)\n",
    "test_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='test', download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNetwork()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=0.1, weight_decay=0)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Batch: 32  Loss: 4.050589561462402\n",
      "Epoch: 0  Batch: 64  Loss: 3.7850289344787598\n",
      "Epoch: 0  Batch: 96  Loss: 2.818643093109131\n",
      "Epoch: 0  Batch: 128  Loss: 2.791229724884033\n",
      "Epoch: 0  Batch: 160  Loss: 2.3206562995910645\n",
      "Epoch: 0  Batch: 192  Loss: 2.3518242835998535\n",
      "Epoch: 0 Validation Loss: 3.8266478553414345\n",
      "Epoch: 1  Batch: 32  Loss: 1.7648658752441406\n",
      "Epoch: 1  Batch: 64  Loss: 1.9724292755126953\n",
      "Epoch: 1  Batch: 96  Loss: 1.6058084964752197\n",
      "Epoch: 1  Batch: 128  Loss: 1.738288164138794\n",
      "Epoch: 1  Batch: 160  Loss: 1.413309097290039\n",
      "Epoch: 1  Batch: 192  Loss: 2.013526439666748\n",
      "Epoch: 1 Validation Loss: 3.871054269373417\n",
      "Epoch: 2  Batch: 32  Loss: 1.4428693056106567\n",
      "Epoch: 2  Batch: 64  Loss: 0.9920452237129211\n",
      "Epoch: 2  Batch: 96  Loss: 1.0038779973983765\n",
      "Epoch: 2  Batch: 128  Loss: 1.2777912616729736\n",
      "Epoch: 2  Batch: 160  Loss: 1.1165809631347656\n",
      "Epoch: 2  Batch: 192  Loss: 0.5623410940170288\n",
      "Epoch: 2 Validation Loss: 3.413864638656378\n",
      "Epoch: 3  Batch: 32  Loss: 0.753483772277832\n",
      "Epoch: 3  Batch: 64  Loss: 0.713233470916748\n",
      "Epoch: 3  Batch: 96  Loss: 0.6236283779144287\n",
      "Epoch: 3  Batch: 128  Loss: 0.7310944199562073\n",
      "Epoch: 3  Batch: 160  Loss: 0.906360387802124\n",
      "Epoch: 3  Batch: 192  Loss: 0.6166670918464661\n",
      "Epoch: 3 Validation Loss: 3.3835762180387974\n",
      "Epoch: 4  Batch: 32  Loss: 0.45609626173973083\n",
      "Epoch: 4  Batch: 64  Loss: 0.6619129180908203\n",
      "Epoch: 4  Batch: 96  Loss: 0.5520175695419312\n",
      "Epoch: 4  Batch: 128  Loss: 0.6765148043632507\n",
      "Epoch: 4  Batch: 160  Loss: 0.5036594271659851\n",
      "Epoch: 4  Batch: 192  Loss: 1.5160794258117676\n",
      "Epoch: 4 Validation Loss: 5.073960654437542\n",
      "Epoch: 5  Batch: 32  Loss: 0.20013396441936493\n",
      "Epoch: 5  Batch: 64  Loss: 0.3394613564014435\n",
      "Epoch: 5  Batch: 96  Loss: 0.39204514026641846\n",
      "Epoch: 5  Batch: 128  Loss: 0.5164495706558228\n",
      "Epoch: 5  Batch: 160  Loss: 0.4044860899448395\n",
      "Epoch: 5  Batch: 192  Loss: 1.7874747514724731\n",
      "Epoch: 5 Validation Loss: 4.286118529736996\n",
      "Epoch: 6  Batch: 32  Loss: 0.18143713474273682\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Variables To Tracks Things\n",
    "epochs = EPOCHS\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = EPOCH_PATIENCE  # How many epochs to wait after val loss has stopped improving\n",
    "min_val_loss = float('inf')  # Initialize to infinity\n",
    "stale_epochs = 0  # Counter for epochs without improvement\n",
    "\n",
    "# For Loop of Epochs\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "\n",
    "    # Update the train_loader with the new transformations\n",
    "    # current_transform = update_transform(i)  # Get the updated transform for the current epoch\n",
    "    # train_data.transform = current_transform  # Update the transform in the dataset\n",
    "    # train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)  # Recreate the DataLoader with the updated dataset\n",
    "\n",
    "    # Train\n",
    "    for b, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        y_pred = model(X_train)  # get predicted values from the training set. Not flattened 2D\n",
    "        loss = criterion(y_pred, y_train)  # how off are we? Compare the predictions to correct answers in y_train\n",
    "\n",
    "        predicted = torch.max(y_pred.data, 1)[\n",
    "            1]  # add up the number of correct predictions. Indexed off the first point\n",
    "        batch_corr = (predicted == y_train).sum()  # how many we got correct from this batch. True = 1, False=0, sum those up\n",
    "        trn_corr += batch_corr  # keep track as we go along in training.\n",
    "\n",
    "        # Update our parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            # Print out some results\n",
    "        if b % BATCH_SIZE == 0:\n",
    "            print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            val_loss += criterion(y_val, y_test).item()  # Sum up the loss from each batch\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Calculate the average loss\n",
    "    loss = criterion(y_val, y_test)\n",
    "\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        stale_epochs = 0  # Reset the stale epochs counter\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        stale_epochs += 1  # Increment the stale epochs counter\n",
    "        if stale_epochs >= patience:\n",
    "            print(f'Stopping early at epoch {i} due to overfitting.')\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break  # Break out of the loop\n",
    "\n",
    "    print(f'Epoch: {i} Validation Loss: {avg_val_loss}')\n",
    "\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'Training Took: {total / 60} minutes!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GPU tensors to CPU tensors, detach them from the computation graph, and then to NumPy arrays\n",
    "train_losses = [tl.cpu().detach().numpy() for tl in train_losses]\n",
    "test_losses = [tl.cpu().detach().numpy() for tl in test_losses]\n",
    "\n",
    "# Now you can plot using matplotlib\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(test_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss at Epoch\")\n",
    "plt.legend()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([t.cpu()/30 for t in train_correct], label=\"Training Accuracy\")\n",
    "plt.plot([t.cpu()/10 for t in test_correct], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy at the end of each Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_load_everything = DataLoader(test_data, batch_size=512, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_load_everything:\n",
    "        X_test, y_test = X_test.to('mps'), y_test.to('mps')\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.max(y_val, 1)[1]\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        total += y_test.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the model on the test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
