{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "EPOCH_PATIENCE = 25\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.0001\n",
    "NUM_CLASSES = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the PyTorch model with BatchNorm2d\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # BatchNorm for the first conv layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # BatchNorm for the second conv layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(128)  # BatchNorm for the third conv layer\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm2d(256)  # BatchNorm for the fourth conv layer\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(256 * 12 * 12, 1024)  # Adjust input features to match your final conv layer output\n",
    "        self.dropout = nn.Dropout(0.35)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, NUM_CLASSES)  # Adjust output features to match your number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with activation, batch normalization, and pooling\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        # Flatten the tensor for dense layers\n",
    "        x = x.view(-1, 256 * 12 * 12)  # Adjust to match your final conv layer output\n",
    "        \n",
    "        # Dense layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transform(epoch):\n",
    "    new_transform = copy.deepcopy(train_transform_1)\n",
    "    \n",
    "    # Basic transformations\n",
    "    if epoch > 10:\n",
    "        new_transform.transforms.insert(0, RandomHorizontalFlip(p=0.5))\n",
    "    if epoch > 15:\n",
    "        new_transform.transforms.insert(0, RandomVerticalFlip(p=0.5))\n",
    "        new_transform.transforms.insert(1, RandomRotation(degrees=15))  # Adjusted rotation\n",
    "    if epoch > 20:\n",
    "        new_transform.transforms[1] = RandomRotation(degrees=45)  # Gradual increase in rotation\n",
    "    # Intermediate transformations\n",
    "    if epoch > 30:\n",
    "        new_transform.transforms.insert(4, RandomAffine(degrees=0, translate=(0.1, 0.1)))  # Gradual affine\n",
    "    if epoch > 40:\n",
    "        new_transform.transforms.insert(5, RandomPerspective(distortion_scale=0.2, p=0.2))  # Gradual perspective\n",
    "    \n",
    "    return new_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_1 = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_2 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "    RandomRotation(degrees=90),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_3 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)),\n",
    "    RandomRotation(degrees=45),\n",
    "    RandomVerticalFlip(p=0.8),\n",
    "    RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_4 = transforms.Compose([\n",
    "    RandomVerticalFlip(),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(20),\n",
    "    RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    RandomAdjustSharpness(0.3),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_1)\n",
    "train_data_2_1 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_2)\n",
    "train_data_2_2 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_2)\n",
    "train_data_3 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_3)\n",
    "train_data_4 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_4)\n",
    "train_data = train_data_4 + train_data_1 + train_data_2_1 + train_data_2_2\n",
    "val_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='val', download=True, transform=val_transform)\n",
    "test_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='test', download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    # Wrap the model with nn.DataParallel\n",
    "    model = nn.DataParallel(model)\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Batch: 64  Loss: 4.715804576873779\n",
      "Epoch: 0 Validation Loss: 4.646520346403122\n",
      "Epoch: 1  Batch: 64  Loss: 4.6035003662109375\n",
      "Epoch: 1 Validation Loss: 4.635409265756607\n",
      "Epoch: 2  Batch: 64  Loss: 4.581812381744385\n",
      "Epoch: 2 Validation Loss: 4.628319084644318\n",
      "Epoch: 3  Batch: 64  Loss: 4.645257472991943\n",
      "Epoch: 3 Validation Loss: 4.618189245462418\n",
      "Epoch: 4  Batch: 64  Loss: 4.6806511878967285\n",
      "Epoch: 4 Validation Loss: 4.602005690336227\n",
      "Epoch: 5  Batch: 64  Loss: 4.6247878074646\n",
      "Epoch: 5 Validation Loss: 4.569271951913834\n",
      "Epoch: 6  Batch: 64  Loss: 4.556412220001221\n",
      "Epoch: 6 Validation Loss: 4.529204219579697\n",
      "Epoch: 7  Batch: 64  Loss: 4.583431243896484\n",
      "Epoch: 7 Validation Loss: 4.496915310621262\n",
      "Epoch: 8  Batch: 64  Loss: 4.47130012512207\n",
      "Epoch: 8 Validation Loss: 4.473915874958038\n",
      "Epoch: 9  Batch: 64  Loss: 4.466860294342041\n",
      "Epoch: 9 Validation Loss: 4.444401949644089\n",
      "Epoch: 10  Batch: 64  Loss: 4.45035457611084\n",
      "Epoch: 10 Validation Loss: 4.39795845746994\n",
      "Epoch: 11  Batch: 64  Loss: 4.304766654968262\n",
      "Epoch: 11 Validation Loss: 4.370901390910149\n",
      "Epoch: 12  Batch: 64  Loss: 4.23718786239624\n",
      "Epoch: 12 Validation Loss: 4.373694121837616\n",
      "Epoch: 13  Batch: 64  Loss: 4.261642932891846\n",
      "Epoch: 13 Validation Loss: 4.316941857337952\n",
      "Epoch: 14  Batch: 64  Loss: 4.121307373046875\n",
      "Epoch: 14 Validation Loss: 4.3114007115364075\n",
      "Epoch: 15  Batch: 64  Loss: 4.091090679168701\n",
      "Epoch: 15 Validation Loss: 4.3486335426568985\n",
      "Epoch: 16  Batch: 64  Loss: 4.02061128616333\n",
      "Epoch: 16 Validation Loss: 4.261346995830536\n",
      "Epoch: 17  Batch: 64  Loss: 3.993190050125122\n",
      "Epoch: 17 Validation Loss: 4.22120264172554\n",
      "Epoch: 18  Batch: 64  Loss: 4.0124030113220215\n",
      "Epoch: 18 Validation Loss: 4.221594616770744\n",
      "Epoch: 19  Batch: 64  Loss: 4.150827884674072\n",
      "Epoch: 19 Validation Loss: 4.215900212526321\n",
      "Epoch: 20  Batch: 64  Loss: 3.9116694927215576\n",
      "Epoch: 20 Validation Loss: 4.198187738656998\n",
      "Epoch: 21  Batch: 64  Loss: 3.9710216522216797\n",
      "Epoch: 21 Validation Loss: 4.155533000826836\n",
      "Epoch: 22  Batch: 64  Loss: 3.9978320598602295\n",
      "Epoch: 22 Validation Loss: 4.150579661130905\n",
      "Epoch: 23  Batch: 64  Loss: 3.865102529525757\n",
      "Epoch: 23 Validation Loss: 4.116996631026268\n",
      "Epoch: 24  Batch: 64  Loss: 3.900819778442383\n",
      "Epoch: 24 Validation Loss: 4.084600120782852\n",
      "Epoch: 25  Batch: 64  Loss: 3.7236697673797607\n",
      "Epoch: 25 Validation Loss: 4.037521228194237\n",
      "Epoch: 26  Batch: 64  Loss: 3.6460273265838623\n",
      "Epoch: 26 Validation Loss: 4.06664577126503\n",
      "Epoch: 27  Batch: 64  Loss: 3.5134449005126953\n",
      "Epoch: 27 Validation Loss: 4.075880810618401\n",
      "Epoch: 28  Batch: 64  Loss: 3.6366913318634033\n",
      "Epoch: 28 Validation Loss: 4.020282581448555\n",
      "Epoch: 29  Batch: 64  Loss: 3.8057680130004883\n",
      "Epoch: 29 Validation Loss: 4.054550141096115\n",
      "Epoch: 30  Batch: 64  Loss: 3.757711410522461\n",
      "Epoch: 30 Validation Loss: 4.007300093770027\n",
      "Epoch: 31  Batch: 64  Loss: 3.7002601623535156\n",
      "Epoch: 31 Validation Loss: 4.0298197120428085\n",
      "Epoch: 32  Batch: 64  Loss: 3.5502121448516846\n",
      "Epoch: 32 Validation Loss: 4.032342106103897\n",
      "Epoch: 33  Batch: 64  Loss: 3.431535482406616\n",
      "Epoch: 33 Validation Loss: 3.9848879128694534\n",
      "Epoch: 34  Batch: 64  Loss: 3.446190595626831\n",
      "Epoch: 34 Validation Loss: 3.9802573323249817\n",
      "Epoch: 35  Batch: 64  Loss: 3.5647506713867188\n",
      "Epoch: 35 Validation Loss: 3.950593441724777\n",
      "Epoch: 36  Batch: 64  Loss: 3.5096867084503174\n",
      "Epoch: 36 Validation Loss: 3.9387941509485245\n",
      "Epoch: 37  Batch: 64  Loss: 3.663158655166626\n",
      "Epoch: 37 Validation Loss: 4.005916282534599\n",
      "Epoch: 38  Batch: 64  Loss: 3.3825953006744385\n",
      "Epoch: 38 Validation Loss: 3.962715581059456\n",
      "Epoch: 39  Batch: 64  Loss: 3.556816339492798\n",
      "Epoch: 39 Validation Loss: 3.9352810233831406\n",
      "Epoch: 40  Batch: 64  Loss: 3.546579122543335\n",
      "Epoch: 40 Validation Loss: 3.9639028310775757\n",
      "Epoch: 41  Batch: 64  Loss: 3.605428695678711\n",
      "Epoch: 41 Validation Loss: 3.887066438794136\n",
      "Epoch: 42  Batch: 64  Loss: 3.158442258834839\n",
      "Epoch: 42 Validation Loss: 3.923996329307556\n",
      "Epoch: 43  Batch: 64  Loss: 3.4083919525146484\n",
      "Epoch: 43 Validation Loss: 3.9332748502492905\n",
      "Epoch: 44  Batch: 64  Loss: 3.120457887649536\n",
      "Epoch: 44 Validation Loss: 3.8856000155210495\n",
      "Epoch: 45  Batch: 64  Loss: 3.2268877029418945\n",
      "Epoch: 45 Validation Loss: 3.870762899518013\n",
      "Epoch: 46  Batch: 64  Loss: 3.3708343505859375\n",
      "Epoch: 46 Validation Loss: 3.8510035425424576\n",
      "Epoch: 47  Batch: 64  Loss: 3.4343528747558594\n",
      "Epoch: 47 Validation Loss: 3.880816087126732\n",
      "Epoch: 48  Batch: 64  Loss: 3.174959182739258\n",
      "Epoch: 48 Validation Loss: 3.8688689172267914\n",
      "Epoch: 49  Batch: 64  Loss: 3.4171783924102783\n",
      "Epoch: 49 Validation Loss: 3.8897843062877655\n",
      "Epoch: 50  Batch: 64  Loss: 2.806246757507324\n",
      "Epoch: 50 Validation Loss: 3.8999898731708527\n",
      "Epoch: 51  Batch: 64  Loss: 3.2723348140716553\n",
      "Epoch: 51 Validation Loss: 3.856837749481201\n",
      "Epoch: 52  Batch: 64  Loss: 3.2416412830352783\n",
      "Epoch: 52 Validation Loss: 3.905721753835678\n",
      "Epoch: 53  Batch: 64  Loss: 2.9454634189605713\n",
      "Epoch: 53 Validation Loss: 3.908665180206299\n",
      "Epoch: 54  Batch: 64  Loss: 2.9533793926239014\n",
      "Epoch: 54 Validation Loss: 3.9046749025583267\n",
      "Epoch: 55  Batch: 64  Loss: 2.900857925415039\n",
      "Epoch: 55 Validation Loss: 3.8659022748470306\n",
      "Epoch: 56  Batch: 64  Loss: 2.9082891941070557\n",
      "Epoch: 56 Validation Loss: 3.851157009601593\n",
      "Epoch: 57  Batch: 64  Loss: 3.0666840076446533\n",
      "Epoch: 57 Validation Loss: 3.852454200387001\n",
      "Epoch: 58  Batch: 64  Loss: 2.907191038131714\n",
      "Epoch: 58 Validation Loss: 3.8443049043416977\n",
      "Epoch: 59  Batch: 64  Loss: 3.0103683471679688\n",
      "Epoch: 59 Validation Loss: 3.8386181741952896\n",
      "Epoch: 60  Batch: 64  Loss: 2.8140575885772705\n",
      "Epoch: 60 Validation Loss: 3.8896874487400055\n",
      "Epoch: 61  Batch: 64  Loss: 2.7874059677124023\n",
      "Epoch: 61 Validation Loss: 3.8308666199445724\n",
      "Epoch: 62  Batch: 64  Loss: 2.814303159713745\n",
      "Epoch: 62 Validation Loss: 3.7402870655059814\n",
      "Epoch: 63  Batch: 64  Loss: 3.0761425495147705\n",
      "Epoch: 63 Validation Loss: 3.7849832475185394\n",
      "Epoch: 64  Batch: 64  Loss: 2.956923246383667\n",
      "Epoch: 64 Validation Loss: 3.7827754765748978\n",
      "Epoch: 65  Batch: 64  Loss: 2.9207217693328857\n",
      "Epoch: 65 Validation Loss: 3.8025279343128204\n",
      "Epoch: 66  Batch: 64  Loss: 2.797863006591797\n",
      "Epoch: 66 Validation Loss: 3.9011206179857254\n",
      "Epoch: 67  Batch: 64  Loss: 2.9240970611572266\n",
      "Epoch: 67 Validation Loss: 3.756203770637512\n",
      "Epoch: 68  Batch: 64  Loss: 2.757870674133301\n",
      "Epoch: 68 Validation Loss: 3.802469789981842\n",
      "Epoch: 69  Batch: 64  Loss: 2.5178935527801514\n",
      "Epoch: 69 Validation Loss: 3.887164205312729\n",
      "Epoch: 70  Batch: 64  Loss: 2.6954193115234375\n",
      "Epoch: 70 Validation Loss: 3.8687354922294617\n",
      "Epoch: 71  Batch: 64  Loss: 2.643676996231079\n",
      "Epoch: 71 Validation Loss: 3.7366050630807877\n",
      "Epoch: 72  Batch: 64  Loss: 2.6864242553710938\n",
      "Epoch: 72 Validation Loss: 3.8151772767305374\n",
      "Epoch: 73  Batch: 64  Loss: 2.605989933013916\n",
      "Epoch: 73 Validation Loss: 3.83345428109169\n",
      "Epoch: 74  Batch: 64  Loss: 2.7850303649902344\n",
      "Epoch: 74 Validation Loss: 3.7531224191188812\n",
      "Epoch: 75  Batch: 64  Loss: 2.4626429080963135\n",
      "Epoch: 75 Validation Loss: 3.7743070274591446\n",
      "Epoch: 76  Batch: 64  Loss: 2.8558971881866455\n",
      "Epoch: 76 Validation Loss: 3.8676244020462036\n",
      "Epoch: 77  Batch: 64  Loss: 2.880448579788208\n",
      "Epoch: 77 Validation Loss: 3.773767441511154\n",
      "Epoch: 78  Batch: 64  Loss: 2.630612373352051\n",
      "Epoch: 78 Validation Loss: 3.85552017390728\n",
      "Epoch: 79  Batch: 64  Loss: 2.442619562149048\n",
      "Epoch: 79 Validation Loss: 3.7812694311141968\n",
      "Epoch: 80  Batch: 64  Loss: 2.45481276512146\n",
      "Epoch: 80 Validation Loss: 3.7279398888349533\n",
      "Epoch: 81  Batch: 64  Loss: 2.848081350326538\n",
      "Epoch: 81 Validation Loss: 3.977859675884247\n",
      "Epoch: 82  Batch: 64  Loss: 2.5408599376678467\n",
      "Epoch: 82 Validation Loss: 3.9071945250034332\n",
      "Epoch: 83  Batch: 64  Loss: 2.9788739681243896\n",
      "Epoch: 83 Validation Loss: 3.880176216363907\n",
      "Epoch: 84  Batch: 64  Loss: 2.3979361057281494\n",
      "Epoch: 84 Validation Loss: 3.8517101407051086\n",
      "Epoch: 85  Batch: 64  Loss: 2.706279754638672\n",
      "Epoch: 85 Validation Loss: 3.8986583054065704\n",
      "Epoch: 86  Batch: 64  Loss: 2.6070363521575928\n",
      "Epoch: 86 Validation Loss: 3.7115675061941147\n",
      "Epoch: 87  Batch: 64  Loss: 2.846745491027832\n",
      "Epoch: 87 Validation Loss: 3.7593033611774445\n",
      "Epoch: 88  Batch: 64  Loss: 2.31306791305542\n",
      "Epoch: 88 Validation Loss: 3.8711635023355484\n",
      "Epoch: 89  Batch: 64  Loss: 2.2010915279388428\n",
      "Epoch: 89 Validation Loss: 3.765625983476639\n",
      "Epoch: 90  Batch: 64  Loss: 3.064101457595825\n",
      "Epoch: 90 Validation Loss: 3.688433051109314\n",
      "Epoch: 91  Batch: 64  Loss: 3.003345251083374\n",
      "Epoch: 91 Validation Loss: 3.7814481407403946\n",
      "Epoch: 92  Batch: 64  Loss: 2.7946293354034424\n",
      "Epoch: 92 Validation Loss: 3.785536602139473\n",
      "Epoch: 93  Batch: 64  Loss: 2.1883442401885986\n",
      "Epoch: 93 Validation Loss: 3.7363231629133224\n",
      "Epoch: 94  Batch: 64  Loss: 2.2941067218780518\n",
      "Epoch: 94 Validation Loss: 3.7565283328294754\n",
      "Epoch: 95  Batch: 64  Loss: 2.4945218563079834\n",
      "Epoch: 95 Validation Loss: 3.7927900552749634\n",
      "Epoch: 96  Batch: 64  Loss: 2.160320520401001\n",
      "Epoch: 96 Validation Loss: 3.9177225828170776\n",
      "Epoch: 97  Batch: 64  Loss: 2.334125518798828\n",
      "Epoch: 97 Validation Loss: 3.9018496721982956\n",
      "Epoch: 98  Batch: 64  Loss: 2.4702603816986084\n",
      "Epoch: 98 Validation Loss: 3.8234693855047226\n",
      "Epoch: 99  Batch: 64  Loss: 2.615933895111084\n",
      "Epoch: 99 Validation Loss: 3.937274754047394\n",
      "Epoch: 100  Batch: 64  Loss: 2.3933160305023193\n",
      "Epoch: 100 Validation Loss: 3.7415258437395096\n",
      "Epoch: 101  Batch: 64  Loss: 2.207407236099243\n",
      "Epoch: 101 Validation Loss: 3.804331049323082\n",
      "Epoch: 102  Batch: 64  Loss: 2.5700504779815674\n",
      "Epoch: 102 Validation Loss: 3.7674244940280914\n",
      "Epoch: 103  Batch: 64  Loss: 2.1048502922058105\n",
      "Epoch: 103 Validation Loss: 3.8431392908096313\n",
      "Epoch: 104  Batch: 64  Loss: 1.915427803993225\n",
      "Epoch: 104 Validation Loss: 3.8582290560007095\n",
      "Epoch: 105  Batch: 64  Loss: 2.334130048751831\n",
      "Epoch: 105 Validation Loss: 3.7806852757930756\n",
      "Epoch: 106  Batch: 64  Loss: 2.2957425117492676\n",
      "Epoch: 106 Validation Loss: 3.77385313808918\n",
      "Epoch: 107  Batch: 64  Loss: 2.019113779067993\n",
      "Epoch: 107 Validation Loss: 3.9001916646957397\n",
      "Epoch: 108  Batch: 64  Loss: 2.211615800857544\n",
      "Epoch: 108 Validation Loss: 3.7938925325870514\n",
      "Epoch: 109  Batch: 64  Loss: 2.3938817977905273\n",
      "Epoch: 109 Validation Loss: 3.9411626011133194\n",
      "Epoch: 110  Batch: 64  Loss: 2.029817581176758\n",
      "Epoch: 110 Validation Loss: 3.817255914211273\n",
      "Epoch: 111  Batch: 64  Loss: 2.2757515907287598\n",
      "Epoch: 111 Validation Loss: 3.8365809470415115\n",
      "Epoch: 112  Batch: 64  Loss: 2.3862674236297607\n",
      "Epoch: 112 Validation Loss: 3.8380611538887024\n",
      "Epoch: 113  Batch: 64  Loss: 1.677162766456604\n",
      "Epoch: 113 Validation Loss: 3.7715452015399933\n",
      "Epoch: 114  Batch: 64  Loss: 2.6589910984039307\n",
      "Epoch: 114 Validation Loss: 3.67632058262825\n",
      "Epoch: 115  Batch: 64  Loss: 2.4774491786956787\n",
      "Epoch: 115 Validation Loss: 3.8506835252046585\n",
      "Epoch: 116  Batch: 64  Loss: 2.5744802951812744\n",
      "Epoch: 116 Validation Loss: 3.843769684433937\n",
      "Epoch: 117  Batch: 64  Loss: 2.0609052181243896\n",
      "Epoch: 117 Validation Loss: 3.8788189738988876\n",
      "Epoch: 118  Batch: 64  Loss: 2.269016981124878\n",
      "Epoch: 118 Validation Loss: 4.008965879678726\n",
      "Epoch: 119  Batch: 64  Loss: 2.2315282821655273\n",
      "Epoch: 119 Validation Loss: 3.8357051014900208\n",
      "Epoch: 120  Batch: 64  Loss: 1.9832528829574585\n",
      "Epoch: 120 Validation Loss: 3.9396467804908752\n",
      "Epoch: 121  Batch: 64  Loss: 2.073845624923706\n",
      "Epoch: 121 Validation Loss: 3.8183467388153076\n",
      "Epoch: 122  Batch: 64  Loss: 2.504173517227173\n",
      "Epoch: 122 Validation Loss: 3.963973358273506\n",
      "Epoch: 123  Batch: 64  Loss: 1.977428913116455\n",
      "Epoch: 123 Validation Loss: 3.891974776983261\n",
      "Epoch: 124  Batch: 64  Loss: 2.563530206680298\n",
      "Epoch: 124 Validation Loss: 3.978758677840233\n",
      "Epoch: 125  Batch: 64  Loss: 2.074198007583618\n",
      "Epoch: 125 Validation Loss: 4.013943135738373\n",
      "Epoch: 126  Batch: 64  Loss: 1.9867305755615234\n",
      "Epoch: 126 Validation Loss: 4.021650746464729\n",
      "Epoch: 127  Batch: 64  Loss: 1.7662229537963867\n",
      "Epoch: 127 Validation Loss: 3.9063572138547897\n",
      "Epoch: 128  Batch: 64  Loss: 2.4303979873657227\n",
      "Epoch: 128 Validation Loss: 3.911720350384712\n",
      "Epoch: 129  Batch: 64  Loss: 2.1796715259552\n",
      "Epoch: 129 Validation Loss: 3.9228430539369583\n",
      "Epoch: 130  Batch: 64  Loss: 2.0900707244873047\n",
      "Epoch: 130 Validation Loss: 3.87678425014019\n",
      "Epoch: 131  Batch: 64  Loss: 2.4934003353118896\n",
      "Epoch: 131 Validation Loss: 3.8633975237607956\n",
      "Epoch: 132  Batch: 64  Loss: 2.13253116607666\n",
      "Epoch: 132 Validation Loss: 3.834747925400734\n",
      "Epoch: 133  Batch: 64  Loss: 2.1598308086395264\n",
      "Epoch: 133 Validation Loss: 3.9026116132736206\n",
      "Epoch: 134  Batch: 64  Loss: 1.8375142812728882\n",
      "Epoch: 134 Validation Loss: 3.929501697421074\n",
      "Epoch: 135  Batch: 64  Loss: 1.8219575881958008\n",
      "Epoch: 135 Validation Loss: 3.9613089859485626\n",
      "Epoch: 136  Batch: 64  Loss: 2.216939687728882\n",
      "Epoch: 136 Validation Loss: 3.9566609263420105\n",
      "Epoch: 137  Batch: 64  Loss: 1.5802167654037476\n",
      "Epoch: 137 Validation Loss: 3.881388634443283\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Variables To Tracks Things\n",
    "epochs = EPOCHS\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = EPOCH_PATIENCE  # How many epochs to wait after val loss has stopped improving\n",
    "min_val_loss = float('inf')  # Initialize to infinity\n",
    "stale_epochs = 0  # Counter for epochs without improvement\n",
    "\n",
    "# For Loop of Epochs\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "\n",
    "    # Update the train_loader with the new transformations\n",
    "    # current_transform = update_transform(i)  # Get the updated transform for the current epoch\n",
    "    # train_data.transform = current_transform  # Update the transform in the dataset\n",
    "    # train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)  # Recreate the DataLoader with the updated dataset\n",
    "\n",
    "    # Train\n",
    "    for b, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        y_pred = model(X_train)  # get predicted values from the training set. Not flattened 2D\n",
    "        loss = criterion(y_pred, y_train)  # how off are we? Compare the predictions to correct answers in y_train\n",
    "\n",
    "        predicted = torch.max(y_pred.data, 1)[\n",
    "            1]  # add up the number of correct predictions. Indexed off the first point\n",
    "        batch_corr = (predicted == y_train).sum()  # how many we got correct from this batch. True = 1, False=0, sum those up\n",
    "        trn_corr += batch_corr  # keep track as we go along in training.\n",
    "\n",
    "        # Update our parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            # Print out some results\n",
    "        if b % BATCH_SIZE == 0:\n",
    "            print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            val_loss += criterion(y_val, y_test).item()  # Sum up the loss from each batch\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Calculate the average loss\n",
    "    loss = criterion(y_val, y_test)\n",
    "\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        stale_epochs = 0  # Reset the stale epochs counter\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        stale_epochs += 1  # Increment the stale epochs counter\n",
    "        if stale_epochs >= patience:\n",
    "            print(f'Stopping early at epoch {i} due to overfitting.')\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break  # Break out of the loop\n",
    "\n",
    "    print(f'Epoch: {i} Validation Loss: {avg_val_loss}')\n",
    "\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'Training Took: {total / 60} minutes!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GPU tensors to CPU tensors, detach them from the computation graph, and then to NumPy arrays\n",
    "train_losses = [tl.cpu().detach().numpy() for tl in train_losses]\n",
    "test_losses = [tl.cpu().detach().numpy() for tl in test_losses]\n",
    "\n",
    "# Now you can plot using matplotlib\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(test_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss at Epoch\")\n",
    "plt.legend()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([t.cpu()/10 for t in train_correct], label=\"Training Accuracy\")\n",
    "plt.plot([t.cpu()/10 for t in test_correct], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy at the end of each Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_load_everything = DataLoader(test_data, batch_size=512, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_load_everything:\n",
    "        X_test, y_test = X_test.to('mps'), y_test.to('mps')\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.max(y_val, 1)[1]\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        total += y_test.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the model on the test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
