{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "EPOCH_PATIENCE = 25\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.001\n",
    "NUM_CLASSES = 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=36864, out_features=1024, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc3): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc4): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc5): Linear(in_features=128, out_features=102, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the PyTorch model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3)\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(256 * 12 * 12, 1024)  # Adjust input features to match your final conv layer output\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, NUM_CLASSES)  # Adjust output features to match your number of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers with activation and pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        \n",
    "        # Flatten the tensor for dense layers\n",
    "        # print(x.shape)\n",
    "        x = x.view(-1, 256 * 12 * 12)  # Adjust to match your final conv layer output\n",
    "        \n",
    "        # Dense layers with dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "cnn_classifier = CNNClassifier()\n",
    "print(cnn_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transform(epoch):\n",
    "    new_transform = copy.deepcopy(train_transform_1)\n",
    "    \n",
    "    # Basic transformations\n",
    "    if epoch > 10:\n",
    "        new_transform.transforms.insert(0, RandomHorizontalFlip(p=0.5))\n",
    "    if epoch > 15:\n",
    "        new_transform.transforms.insert(0, RandomVerticalFlip(p=0.5))\n",
    "        new_transform.transforms.insert(1, RandomRotation(degrees=15))  # Adjusted rotation\n",
    "    if epoch > 20:\n",
    "        new_transform.transforms[1] = RandomRotation(degrees=45)  # Gradual increase in rotation\n",
    "    # Intermediate transformations\n",
    "    if epoch > 30:\n",
    "        new_transform.transforms.insert(4, RandomAffine(degrees=0, translate=(0.1, 0.1)))  # Gradual affine\n",
    "    if epoch > 40:\n",
    "        new_transform.transforms.insert(5, RandomPerspective(distortion_scale=0.2, p=0.2))  # Gradual perspective\n",
    "    \n",
    "    return new_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_1 = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_2 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.0)),\n",
    "    RandomRotation(degrees=90),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_3 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)),\n",
    "    RandomRotation(degrees=45),\n",
    "    RandomVerticalFlip(p=0.8),\n",
    "    RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_4 = transforms.Compose([\n",
    "    RandomVerticalFlip(),\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(20),\n",
    "    RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "    RandomAdjustSharpness(0.3),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_1)\n",
    "train_data_2 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_2)\n",
    "train_data_3 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_3)\n",
    "train_data_4 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_4)\n",
    "train_data = train_data_4\n",
    "val_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='val', download=True, transform=val_transform)\n",
    "test_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='test', download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_classifier.parameters(), lr=LR)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Batch: 32  Loss: 4.6414361000061035\n",
      "Epoch: 0 Validation Loss: 4.625821024179459\n",
      "Epoch: 1  Batch: 32  Loss: 4.6321892738342285\n",
      "Epoch: 1 Validation Loss: 4.626286715269089\n",
      "Epoch: 2  Batch: 32  Loss: 4.63336181640625\n",
      "Epoch: 2 Validation Loss: 4.625667482614517\n",
      "Epoch: 3  Batch: 32  Loss: 4.607504844665527\n",
      "Epoch: 3 Validation Loss: 4.626364782452583\n",
      "Epoch: 4  Batch: 32  Loss: 4.624454975128174\n",
      "Epoch: 4 Validation Loss: 4.626711845397949\n",
      "Epoch: 5  Batch: 32  Loss: 4.636246681213379\n",
      "Epoch: 5 Validation Loss: 4.626743927598\n",
      "Epoch: 6  Batch: 32  Loss: 4.621623992919922\n",
      "Epoch: 6 Validation Loss: 4.62648805975914\n",
      "Epoch: 7  Batch: 32  Loss: 4.626453876495361\n",
      "Epoch: 7 Validation Loss: 4.625757500529289\n",
      "Epoch: 8  Batch: 32  Loss: 4.632580280303955\n",
      "Epoch: 8 Validation Loss: 4.626574918627739\n",
      "Epoch: 9  Batch: 32  Loss: 4.6491875648498535\n",
      "Epoch: 9 Validation Loss: 4.625297918915749\n",
      "Epoch: 10  Batch: 32  Loss: 4.640514850616455\n",
      "Epoch: 10 Validation Loss: 4.626503512263298\n",
      "Epoch: 11  Batch: 32  Loss: 4.615392208099365\n",
      "Epoch: 11 Validation Loss: 4.626521214842796\n",
      "Epoch: 12  Batch: 32  Loss: 4.635197162628174\n",
      "Epoch: 12 Validation Loss: 4.627135306596756\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Variables To Tracks Things\n",
    "epochs = EPOCHS\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = EPOCH_PATIENCE  # How many epochs to wait after val loss has stopped improving\n",
    "min_val_loss = float('inf')  # Initialize to infinity\n",
    "stale_epochs = 0  # Counter for epochs without improvement\n",
    "\n",
    "# For Loop of Epochs\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "\n",
    "    # Update the train_loader with the new transformations\n",
    "    # current_transform = update_transform(i)  # Get the updated transform for the current epoch\n",
    "    # train_data.transform = current_transform  # Update the transform in the dataset\n",
    "    # train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)  # Recreate the DataLoader with the updated dataset\n",
    "\n",
    "    # Train\n",
    "    for b, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        y_pred = model(X_train)  # get predicted values from the training set. Not flattened 2D\n",
    "        loss = criterion(y_pred, y_train)  # how off are we? Compare the predictions to correct answers in y_train\n",
    "\n",
    "        predicted = torch.max(y_pred.data, 1)[\n",
    "            1]  # add up the number of correct predictions. Indexed off the first point\n",
    "        batch_corr = (predicted == y_train).sum()  # how many we got correct from this batch. True = 1, False=0, sum those up\n",
    "        trn_corr += batch_corr  # keep track as we go along in training.\n",
    "\n",
    "        # Update our parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            # Print out some results\n",
    "        if b % BATCH_SIZE == 0:\n",
    "            print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            val_loss += criterion(y_val, y_test).item()  # Sum up the loss from each batch\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Calculate the average loss\n",
    "    loss = criterion(y_val, y_test)\n",
    "\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        stale_epochs = 0  # Reset the stale epochs counter\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        stale_epochs += 1  # Increment the stale epochs counter\n",
    "        if stale_epochs >= patience:\n",
    "            print(f'Stopping early at epoch {i} due to overfitting.')\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break  # Break out of the loop\n",
    "\n",
    "    print(f'Epoch: {i} Validation Loss: {avg_val_loss}')\n",
    "\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'Training Took: {total / 60} minutes!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GPU tensors to CPU tensors, detach them from the computation graph, and then to NumPy arrays\n",
    "train_losses = [tl.cpu().detach().numpy() for tl in train_losses]\n",
    "test_losses = [tl.cpu().detach().numpy() for tl in test_losses]\n",
    "\n",
    "# Now you can plot using matplotlib\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(test_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss at Epoch\")\n",
    "plt.legend()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([t.cpu()/30 for t in train_correct], label=\"Training Accuracy\")\n",
    "plt.plot([t.cpu()/10 for t in test_correct], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy at the end of each Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_load_everything = DataLoader(test_data, batch_size=512, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_load_everything:\n",
    "        X_test, y_test = X_test.to('mps'), y_test.to('mps')\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.max(y_val, 1)[1]\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        total += y_test.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the model on the test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
