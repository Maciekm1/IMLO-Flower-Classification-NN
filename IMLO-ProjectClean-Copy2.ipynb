{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "EPOCH_PATIENCE = 10\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 102\n",
    "WEIGHT_DECAY = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(num_features=32)\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=64)\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=128)\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=256)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=256 * 14 * 14, out_features=1024)\n",
    "        self.fc2 = nn.Linear(in_features=1024, out_features=NUM_CLASSES)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
    "        \n",
    "        x = x.view(-1, 256 * 14 * 14)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_transform(epoch):\n",
    "    new_transform = copy.deepcopy(train_transform_1)\n",
    "    \n",
    "    # Basic transformations\n",
    "    if epoch > 10:\n",
    "        new_transform.transforms.insert(0, RandomHorizontalFlip(p=0.5))\n",
    "    if epoch > 15:\n",
    "        new_transform.transforms.insert(0, RandomVerticalFlip(p=0.5))\n",
    "        new_transform.transforms.insert(1, RandomRotation(degrees=15))  # Adjusted rotation\n",
    "    if epoch > 20:\n",
    "        new_transform.transforms[1] = RandomRotation(degrees=45)  # Gradual increase in rotation\n",
    "    # Intermediate transformations\n",
    "    if epoch > 30:\n",
    "        new_transform.transforms.insert(4, RandomAffine(degrees=0, translate=(0.1, 0.1)))  # Gradual affine\n",
    "    if epoch > 40:\n",
    "        new_transform.transforms.insert(5, RandomPerspective(distortion_scale=0.2, p=0.2))  # Gradual perspective\n",
    "    \n",
    "    return new_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform_1 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_2 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.8, 1.2)),\n",
    "    RandomRotation(degrees=15),\n",
    "    RandomHorizontalFlip(p=0.8),\n",
    "    RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "train_transform_3 = Compose([\n",
    "    Resize((256, 256)),\n",
    "    RandomResizedCrop(size=(224, 224), scale=(0.6, 1.0)),\n",
    "    RandomRotation(degrees=45),\n",
    "    RandomVerticalFlip(p=0.8),\n",
    "    RandomPerspective(distortion_scale=0.2, p=0.2),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175])\n",
    "])\n",
    "\n",
    "val_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])\n",
    "\n",
    "test_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.4330, 0.3819, 0.2964], std=[0.2555, 0.2056, 0.2175]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_1 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_1)\n",
    "train_data_2 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_2)\n",
    "train_data_3 = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='train', download=True, transform=train_transform_3)\n",
    "train_data = train_data_1 + train_data_2 + train_data_3\n",
    "val_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='val', download=True, transform=val_transform)\n",
    "test_data = datasets.Flowers102(root=\"/Users/maciek/cnn_data\", split='test', download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvolutionalNetwork()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Batch: 32  Loss: 4.573278427124023\n",
      "Epoch: 0  Batch: 64  Loss: 4.642066478729248\n",
      "Epoch: 0  Batch: 96  Loss: 4.346078395843506\n",
      "Epoch: 0 Validation Loss: 4.624080866575241\n",
      "Epoch: 1  Batch: 32  Loss: 4.532670974731445\n",
      "Epoch: 1  Batch: 64  Loss: 4.250118255615234\n",
      "Epoch: 1  Batch: 96  Loss: 4.684528350830078\n",
      "Epoch: 1 Validation Loss: 4.557657927274704\n",
      "Epoch: 2  Batch: 32  Loss: 4.119859218597412\n",
      "Epoch: 2  Batch: 64  Loss: 4.292809963226318\n",
      "Epoch: 2  Batch: 96  Loss: 4.202236175537109\n",
      "Epoch: 2 Validation Loss: 4.49947227537632\n",
      "Epoch: 3  Batch: 32  Loss: 4.236835479736328\n",
      "Epoch: 3  Batch: 64  Loss: 4.252809524536133\n",
      "Epoch: 3  Batch: 96  Loss: 4.359429359436035\n",
      "Epoch: 3 Validation Loss: 4.482351645827293\n",
      "Epoch: 4  Batch: 32  Loss: 4.067099571228027\n",
      "Epoch: 4  Batch: 64  Loss: 4.019387245178223\n",
      "Epoch: 4  Batch: 96  Loss: 4.4176201820373535\n",
      "Epoch: 4 Validation Loss: 4.496057339012623\n",
      "Epoch: 5  Batch: 32  Loss: 4.174543380737305\n",
      "Epoch: 5  Batch: 64  Loss: 4.109417915344238\n",
      "Epoch: 5  Batch: 96  Loss: 4.58276891708374\n",
      "Epoch: 5 Validation Loss: 4.570131525397301\n",
      "Epoch: 6  Batch: 32  Loss: 4.018731117248535\n",
      "Epoch: 6  Batch: 64  Loss: 3.874803066253662\n",
      "Epoch: 6  Batch: 96  Loss: 4.306430816650391\n",
      "Epoch: 6 Validation Loss: 4.683532997965813\n",
      "Epoch: 7  Batch: 32  Loss: 4.156497001647949\n",
      "Epoch: 7  Batch: 64  Loss: 4.16485071182251\n",
      "Epoch: 7  Batch: 96  Loss: 4.224109649658203\n",
      "Epoch: 7 Validation Loss: 4.618957087397575\n",
      "Epoch: 8  Batch: 32  Loss: 4.047776222229004\n",
      "Epoch: 8  Batch: 64  Loss: 4.163729190826416\n",
      "Epoch: 8  Batch: 96  Loss: 4.096189975738525\n",
      "Epoch: 8 Validation Loss: 4.4313617423176765\n",
      "Epoch: 9  Batch: 32  Loss: 4.138369560241699\n",
      "Epoch: 9  Batch: 64  Loss: 3.9384658336639404\n",
      "Epoch: 9  Batch: 96  Loss: 4.257285118103027\n",
      "Epoch: 9 Validation Loss: 4.50247959792614\n",
      "Epoch: 10  Batch: 32  Loss: 4.069369316101074\n",
      "Epoch: 10  Batch: 64  Loss: 4.160267353057861\n",
      "Epoch: 10  Batch: 96  Loss: 4.148343086242676\n",
      "Epoch: 10 Validation Loss: 4.4501773416996\n",
      "Epoch: 11  Batch: 32  Loss: 3.9732422828674316\n",
      "Epoch: 11  Batch: 64  Loss: 4.209992408752441\n",
      "Epoch: 11  Batch: 96  Loss: 3.928382158279419\n",
      "Epoch: 11 Validation Loss: 4.534465171396732\n",
      "Epoch: 12  Batch: 32  Loss: 4.044185161590576\n",
      "Epoch: 12  Batch: 64  Loss: 4.449828147888184\n",
      "Epoch: 12  Batch: 96  Loss: 4.059170722961426\n",
      "Epoch: 12 Validation Loss: 4.5844739228487015\n",
      "Epoch: 13  Batch: 32  Loss: 4.130155563354492\n",
      "Epoch: 13  Batch: 64  Loss: 4.117843151092529\n",
      "Epoch: 13  Batch: 96  Loss: 4.006655693054199\n",
      "Epoch: 13 Validation Loss: 4.667756974697113\n",
      "Epoch: 14  Batch: 32  Loss: 4.189050674438477\n",
      "Epoch: 14  Batch: 64  Loss: 4.254061698913574\n",
      "Epoch: 14  Batch: 96  Loss: 3.7157998085021973\n",
      "Epoch: 14 Validation Loss: 4.709678195416927\n",
      "Epoch: 15  Batch: 32  Loss: 3.9088168144226074\n",
      "Epoch: 15  Batch: 64  Loss: 4.115403175354004\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b, (X_train, y_train) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 29\u001b[0m     X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, y_train\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(X_train)  \u001b[38;5;66;03m# get predicted values from the training set. Not flattened 2D\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_train)  \u001b[38;5;66;03m# how off are we? Compare the predictions to correct answers in y_train\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Create Variables To Tracks Things\n",
    "epochs = EPOCHS\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "# Early Stopping Parameters\n",
    "patience = EPOCH_PATIENCE  # How many epochs to wait after val loss has stopped improving\n",
    "min_val_loss = float('inf')  # Initialize to infinity\n",
    "stale_epochs = 0  # Counter for epochs without improvement\n",
    "\n",
    "# For Loop of Epochs\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "\n",
    "    # Update the train_loader with the new transformations\n",
    "    current_transform = update_transform(i)  # Get the updated transform for the current epoch\n",
    "    train_data.transform = current_transform  # Update the transform in the dataset\n",
    "    train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)  # Recreate the DataLoader with the updated dataset\n",
    "\n",
    "    # Train\n",
    "    for b, (X_train, y_train) in enumerate(train_loader, 1):\n",
    "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "        y_pred = model(X_train)  # get predicted values from the training set. Not flattened 2D\n",
    "        loss = criterion(y_pred, y_train)  # how off are we? Compare the predictions to correct answers in y_train\n",
    "\n",
    "        predicted = torch.max(y_pred.data, 1)[\n",
    "            1]  # add up the number of correct predictions. Indexed off the first point\n",
    "        batch_corr = (predicted == y_train).sum()  # how many we got correct from this batch. True = 1, False=0, sum those up\n",
    "        trn_corr += batch_corr  # keep track as we go along in training.\n",
    "\n",
    "        # Update our parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "            # Print out some results\n",
    "        if b % BATCH_SIZE == 0:\n",
    "            print(f'Epoch: {i}  Batch: {b}  Loss: {loss.item()}')\n",
    "\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(trn_corr)\n",
    "\n",
    "    # Test\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for b, (X_test, y_test) in enumerate(val_loader):\n",
    "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "            val_loss += criterion(y_val, y_test).item()  # Sum up the loss from each batch\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # Calculate the average loss\n",
    "    loss = criterion(y_val, y_test)\n",
    "\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(tst_corr)\n",
    "\n",
    "    # Early Stopping Check\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        stale_epochs = 0  # Reset the stale epochs counter\n",
    "        best_model_state = model.state_dict()\n",
    "    else:\n",
    "        stale_epochs += 1  # Increment the stale epochs counter\n",
    "        if stale_epochs >= patience:\n",
    "            print(f'Stopping early at epoch {i} due to overfitting.')\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break  # Break out of the loop\n",
    "\n",
    "    print(f'Epoch: {i} Validation Loss: {avg_val_loss}')\n",
    "\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'Training Took: {total / 60} minutes!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert GPU tensors to CPU tensors, detach them from the computation graph, and then to NumPy arrays\n",
    "train_losses = [tl.cpu().detach().numpy() for tl in train_losses]\n",
    "test_losses = [tl.cpu().detach().numpy() for tl in test_losses]\n",
    "\n",
    "# Now you can plot using matplotlib\n",
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(test_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss at Epoch\")\n",
    "plt.legend()\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([t.cpu()/30 for t in train_correct], label=\"Training Accuracy\")\n",
    "plt.plot([t.cpu()/10 for t in test_correct], label=\"Validation Accuracy\")\n",
    "plt.title(\"Accuracy at the end of each Epoch\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_load_everything = DataLoader(test_data, batch_size=1024, shuffle=False)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_test, y_test in test_load_everything:\n",
    "        X_test, y_test = X_test.to('mps'), y_test.to('mps')\n",
    "        y_val = model(X_test)\n",
    "        predicted = torch.max(y_val, 1)[1]\n",
    "        correct += (predicted == y_test).sum().item()\n",
    "        total += y_test.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy of the model on the test set: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
